{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c414afa4-aad6-433d-9cf5-8ea8b12c0ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello to our notebook\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello to our notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec866eac-e101-4697-b115-09531f324780",
   "metadata": {},
   "source": [
    "## Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9443fbd4-024d-48f9-9268-dc9d331fb68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader #for the pdf file\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78667f4-9aea-4857-b7c6-acd2aab92060",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "721bb9c3-1980-4481-a7b7-3afb17a7a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "352dfedd-03f6-4bbd-b6ca-41039e7aa768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-27 01:55:34--  https://ia800907.us.archive.org/21/items/IndianHerbalRemedies_201903/Handbook%20of%20Medicinal%20Herbs.pdf\n",
      "Resolving ia800907.us.archive.org (ia800907.us.archive.org)... 207.241.233.67\n",
      "Connecting to ia800907.us.archive.org (ia800907.us.archive.org)|207.241.233.67|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8011621 (7.6M) [application/pdf]\n",
      "Saving to: ‘Handbook of Medicinal Herbs.pdf’\n",
      "\n",
      "Handbook of Medicin 100%[===================>]   7.64M   293KB/s    in 36s     \n",
      "\n",
      "2024-07-27 01:56:14 (219 KB/s) - ‘Handbook of Medicinal Herbs.pdf’ saved [8011621/8011621]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd data\n",
    "!wget https://ia800907.us.archive.org/21/items/IndianHerbalRemedies_201903/Handbook%20of%20Medicinal%20Herbs.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44d00c6d-0992-44c6-abde-538de691c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/khaoula1972/Herbal-Medicine-Chatbot-using-Llama2\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f57c0d-91de-46f1-b8ee-875e7c3a7c14",
   "metadata": {},
   "source": [
    "## Pinecone variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7a1cec-ad59-4b4e-a81b-1766916b78bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = \"a4dc4d0f-f863-48fe-bd65-a02a2118e0b8\"\n",
    "# For the environnment, it's no longer necessary to have an environnement according to the pinecone support system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057817d3-be40-4ceb-a03f-de17a2523e40",
   "metadata": {},
   "source": [
    "## Extract data from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ede53bd7-f7e8-40ad-8a67-206f8333c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                   loader_cls=PyPDFLoader)\n",
    "    documents=loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0df8f45a-1cc3-4c8d-829e-219db8d521fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8760d2a2-b100-4fa3-bb28-dac880385432",
   "metadata": {},
   "source": [
    "Now as we already created and extracted the data, we'll need to create text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec476df-9dd1-4440-9ce0-257945b0e881",
   "metadata": {},
   "source": [
    "## Create text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c07eee43-139d-473a-943b-6678006ee62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap = 20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36059382-d49b-43e3-9f28-838f1d674e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of chunks 7466\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"length of chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876437bf-958f-479f-9c4f-4318a5255547",
   "metadata": {},
   "source": [
    "## Vector embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c0c38d2-992e-4af2-8288-9b08ed8f4c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e16cd58e-2112-4e0d-b09b-96ddb76360f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khaoula1972/Herbal-Medicine-Chatbot-using-Llama2/HerbMedBot/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'cached_download' (from 'huggingface_hub.file_download') is deprecated and will be removed from version '0.26'. Use `hf_hub_download` instead.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0217f51f4a49daa534a5805b36bf53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff2f4602aac4df2a1564ba01b109b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e85ffb56aa437daf70e7f37ab25848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ca15bbe3234397a66452c3d8bc7986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb595c190644ddfb7283ee35d880f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80a03c80d3e4ce0ab7936912848e1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402c81012331460985acd4d7b8647f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb504e0dae5c4ceab26fcc1558b49d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5345d6e52313418eb6a77303d0e0eee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0ada542c8543a1b3ca7f5ef89ec310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0317380dfbc49998d13c4024f59b50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427761763e5449a6b7a6965f9639bbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb0074717264f17a841d95b0c6b1ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a63409488564d789991bcf27ea63da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a942ba1da84a8eb2f82506ab592ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163a6ab5f2594717b47a699adf667857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfd3283b-7b84-4ba5-8d68-9ddc5cbd50d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49acc55a-0791-4e7f-93c0-c1b5db43127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "# testing the embedding model\n",
    "query_result = embedding.embed_query(\"Hello world\")\n",
    "print(len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e027e72-6d65-49cc-bad1-216d60622f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.034477315843105316,\n",
       " 0.031023172661662102,\n",
       " 0.006734910886734724,\n",
       " 0.02610892429947853,\n",
       " -0.03936195746064186,\n",
       " -0.1603025197982788,\n",
       " 0.06692396104335785,\n",
       " -0.006441440898925066,\n",
       " -0.04745054617524147,\n",
       " 0.014758836477994919,\n",
       " 0.07087532430887222,\n",
       " 0.055527545511722565,\n",
       " 0.01919332519173622,\n",
       " -0.026251299306750298,\n",
       " -0.01010951679199934,\n",
       " -0.026940451934933662,\n",
       " 0.022307397797703743,\n",
       " -0.022226639091968536,\n",
       " -0.1496926248073578,\n",
       " -0.01749303936958313,\n",
       " 0.007676327601075172,\n",
       " 0.054352276027202606,\n",
       " 0.0032544792629778385,\n",
       " 0.03172592446208,\n",
       " -0.08462144434452057,\n",
       " -0.029405953362584114,\n",
       " 0.05159562826156616,\n",
       " 0.048124104738235474,\n",
       " -0.003314818488433957,\n",
       " -0.05827919766306877,\n",
       " 0.04196928068995476,\n",
       " 0.02221069671213627,\n",
       " 0.12818878889083862,\n",
       " -0.02233896404504776,\n",
       " -0.011656257323920727,\n",
       " 0.06292840093374252,\n",
       " -0.03287629410624504,\n",
       " -0.09122602641582489,\n",
       " -0.031175386160612106,\n",
       " 0.05269954726099968,\n",
       " 0.047034841030836105,\n",
       " -0.08420310169458389,\n",
       " -0.030056146904826164,\n",
       " -0.020744822919368744,\n",
       " 0.009517822414636612,\n",
       " -0.0037217941135168076,\n",
       " 0.007343323435634375,\n",
       " 0.03932430222630501,\n",
       " 0.09327404201030731,\n",
       " -0.003788547357544303,\n",
       " -0.0527421198785305,\n",
       " -0.05805818736553192,\n",
       " -0.006864348892122507,\n",
       " 0.005283175501972437,\n",
       " 0.08289303630590439,\n",
       " 0.019362740218639374,\n",
       " 0.0062845125794410706,\n",
       " -0.010330763645470142,\n",
       " 0.009032384492456913,\n",
       " -0.03768375888466835,\n",
       " -0.04520605504512787,\n",
       " 0.024016302078962326,\n",
       " -0.006944123189896345,\n",
       " 0.013491673395037651,\n",
       " 0.10005496442317963,\n",
       " -0.0716838389635086,\n",
       " -0.021695099771022797,\n",
       " 0.031618449836969376,\n",
       " -0.051634691655635834,\n",
       " -0.08224774152040482,\n",
       " -0.06569334864616394,\n",
       " -0.009895352646708488,\n",
       " 0.005816369317471981,\n",
       " 0.07355460524559021,\n",
       " -0.03405030816793442,\n",
       " 0.02488608844578266,\n",
       " 0.0144880386069417,\n",
       " 0.026457371190190315,\n",
       " 0.00965668074786663,\n",
       " 0.030217230319976807,\n",
       " 0.052803948521614075,\n",
       " -0.07535988092422485,\n",
       " 0.009897170588374138,\n",
       " 0.02983679436147213,\n",
       " 0.01755552366375923,\n",
       " 0.023091983050107956,\n",
       " 0.0019338511629030108,\n",
       " 0.0014002013485878706,\n",
       " -0.047175996005535126,\n",
       " -0.011194296181201935,\n",
       " -0.11420141905546188,\n",
       " -0.01981193758547306,\n",
       " 0.04026622697710991,\n",
       " 0.0021929896902292967,\n",
       " -0.07979217171669006,\n",
       " -0.025382349267601967,\n",
       " 0.09448302537202835,\n",
       " -0.028981097042560577,\n",
       " -0.14500251412391663,\n",
       " 0.23097744584083557,\n",
       " 0.027731193229556084,\n",
       " 0.032111503183841705,\n",
       " 0.031064989045262337,\n",
       " 0.04283284395933151,\n",
       " 0.06423782557249069,\n",
       " 0.03216312825679779,\n",
       " -0.0048767318949103355,\n",
       " 0.05569940432906151,\n",
       " -0.037532366812229156,\n",
       " -0.02150554209947586,\n",
       " -0.028342653065919876,\n",
       " -0.028846969828009605,\n",
       " 0.03835304453969002,\n",
       " -0.017468653619289398,\n",
       " 0.052485376596450806,\n",
       " -0.07487601786851883,\n",
       " -0.03125973790884018,\n",
       " 0.021841607987880707,\n",
       " -0.03989560902118683,\n",
       " -0.00858708843588829,\n",
       " 0.02695661038160324,\n",
       " -0.04849547520279884,\n",
       " 0.01146987359970808,\n",
       " 0.029618192464113235,\n",
       " -0.020572224631905556,\n",
       " 0.013103857636451721,\n",
       " 0.028833499178290367,\n",
       " -3.1941979827962646e-33,\n",
       " 0.064782053232193,\n",
       " -0.018130194395780563,\n",
       " 0.051789894700050354,\n",
       " 0.12198271602392197,\n",
       " 0.028780169785022736,\n",
       " 0.00872202217578888,\n",
       " -0.07052113115787506,\n",
       " -0.01690724864602089,\n",
       " 0.04073973000049591,\n",
       " 0.042116161435842514,\n",
       " 0.02544720284640789,\n",
       " 0.0357462614774704,\n",
       " -0.049144722521305084,\n",
       " 0.0021290285512804985,\n",
       " -0.01554651279002428,\n",
       " 0.05073057860136032,\n",
       " -0.048185352236032486,\n",
       " 0.03588060289621353,\n",
       " -0.004067079164087772,\n",
       " 0.10172474384307861,\n",
       " -0.05597004294395447,\n",
       " -0.010681044310331345,\n",
       " 0.011235790327191353,\n",
       " 0.09068652242422104,\n",
       " 0.004234463442116976,\n",
       " 0.03513864800333977,\n",
       " -0.009702890180051327,\n",
       " -0.09386510401964188,\n",
       " 0.09285558015108109,\n",
       " 0.008004914969205856,\n",
       " -0.0077053699642419815,\n",
       " -0.0520867295563221,\n",
       " -0.012587993405759335,\n",
       " 0.003266903106123209,\n",
       " 0.0060135433450341225,\n",
       " 0.007581562269479036,\n",
       " 0.010517186485230923,\n",
       " -0.08634558320045471,\n",
       " -0.06987879425287247,\n",
       " -0.0025338977575302124,\n",
       " -0.09097658097743988,\n",
       " 0.046887289732694626,\n",
       " 0.05207657441496849,\n",
       " 0.007193864788860083,\n",
       " 0.010903616435825825,\n",
       " -0.005229519214481115,\n",
       " 0.013937349431216717,\n",
       " 0.02196834608912468,\n",
       " 0.034208595752716064,\n",
       " 0.06022465601563454,\n",
       " 0.00011663910117931664,\n",
       " 0.014731943607330322,\n",
       " -0.07008921355009079,\n",
       " 0.028499074280261993,\n",
       " -0.027601728215813637,\n",
       " 0.010768409818410873,\n",
       " 0.03483093902468681,\n",
       " -0.022487878799438477,\n",
       " 0.009769014082849026,\n",
       " 0.07722783088684082,\n",
       " 0.021588340401649475,\n",
       " 0.11495620757341385,\n",
       " -0.0680011585354805,\n",
       " 0.023761006072163582,\n",
       " -0.015983907505869865,\n",
       " -0.017826922237873077,\n",
       " 0.06439490616321564,\n",
       " 0.032025691121816635,\n",
       " 0.050270214676856995,\n",
       " -0.005913726985454559,\n",
       " -0.033708009868860245,\n",
       " 0.01784026063978672,\n",
       " 0.016573289409279823,\n",
       " 0.06329652667045593,\n",
       " 0.034677159041166306,\n",
       " 0.04647349193692207,\n",
       " 0.09790614992380142,\n",
       " -0.006635485216975212,\n",
       " 0.025206999853253365,\n",
       " -0.07798829674720764,\n",
       " 0.016926441341638565,\n",
       " -0.0009458065615035594,\n",
       " 0.02247190847992897,\n",
       " -0.03825321048498154,\n",
       " 0.09570479393005371,\n",
       " -0.005350802093744278,\n",
       " 0.010469064116477966,\n",
       " -0.11524055153131485,\n",
       " -0.01326246652752161,\n",
       " -0.010709411464631557,\n",
       " -0.0831172838807106,\n",
       " 0.07327356189489365,\n",
       " 0.04939223453402519,\n",
       " -0.008994348347187042,\n",
       " -0.09584560245275497,\n",
       " 3.3661485617505796e-33,\n",
       " 0.12493188679218292,\n",
       " 0.0193497147411108,\n",
       " -0.05822574347257614,\n",
       " -0.03598818928003311,\n",
       " -0.05074671283364296,\n",
       " -0.045662373304367065,\n",
       " -0.08260336518287659,\n",
       " 0.14819476008415222,\n",
       " -0.08842120319604874,\n",
       " 0.060274429619312286,\n",
       " 0.05103014409542084,\n",
       " 0.010303168557584286,\n",
       " 0.14121422171592712,\n",
       " 0.030813805758953094,\n",
       " 0.06103309988975525,\n",
       " -0.05285128578543663,\n",
       " 0.1366489678621292,\n",
       " 0.009189908392727375,\n",
       " -0.017325174063444138,\n",
       " -0.012848656624555588,\n",
       " -0.007995286025106907,\n",
       " -0.05098004639148712,\n",
       " -0.052350640296936035,\n",
       " 0.0075930031016469,\n",
       " -0.01516627799719572,\n",
       " 0.016960322856903076,\n",
       " 0.021270515397191048,\n",
       " 0.020558062940835953,\n",
       " -0.12002810835838318,\n",
       " 0.014461780898272991,\n",
       " 0.026759887114167213,\n",
       " 0.025330688804388046,\n",
       " -0.042754627764225006,\n",
       " 0.006768502295017242,\n",
       " -0.014458559453487396,\n",
       " 0.04526197165250778,\n",
       " -0.09147651493549347,\n",
       " -0.019439129158854485,\n",
       " -0.01783343218266964,\n",
       " -0.054910145699977875,\n",
       " -0.052641090005636215,\n",
       " -0.010459056124091148,\n",
       " -0.052016016095876694,\n",
       " 0.020892005413770676,\n",
       " -0.0799703523516655,\n",
       " -0.012111276388168335,\n",
       " -0.05773146077990532,\n",
       " 0.02317822352051735,\n",
       " -0.008031714707612991,\n",
       " -0.02598928101360798,\n",
       " -0.07995672523975372,\n",
       " -0.02072886750102043,\n",
       " 0.0488177090883255,\n",
       " -0.02038913033902645,\n",
       " -0.04917661100625992,\n",
       " 0.014159657061100006,\n",
       " -0.06362202018499374,\n",
       " -0.0078073651529848576,\n",
       " 0.01643151044845581,\n",
       " -0.025682523846626282,\n",
       " 0.013381090946495533,\n",
       " 0.026248810812830925,\n",
       " 0.009978415444493294,\n",
       " 0.06322888284921646,\n",
       " 0.002672172849997878,\n",
       " -0.0065827565267682076,\n",
       " 0.016631929203867912,\n",
       " 0.032366491854190826,\n",
       " 0.03794245049357414,\n",
       " -0.03637605160474777,\n",
       " -0.006910907570272684,\n",
       " 0.00015963036275934428,\n",
       " -0.0016335484106093645,\n",
       " -0.027278142049908638,\n",
       " -0.028038054704666138,\n",
       " 0.04968145489692688,\n",
       " -0.02886715531349182,\n",
       " -0.002418066607788205,\n",
       " 0.014774911105632782,\n",
       " 0.009764587506651878,\n",
       " 0.005797612480819225,\n",
       " 0.013486200012266636,\n",
       " 0.005567916203290224,\n",
       " 0.037227071821689606,\n",
       " 0.00723249139264226,\n",
       " 0.0401562824845314,\n",
       " 0.0815032422542572,\n",
       " 0.07199165970087051,\n",
       " -0.013056200928986073,\n",
       " -0.042882002890110016,\n",
       " -0.0110112139955163,\n",
       " 0.00489781703799963,\n",
       " -0.009229743853211403,\n",
       " 0.03519149869680405,\n",
       " -0.05103500187397003,\n",
       " -1.571437735492509e-08,\n",
       " -0.08862444013357162,\n",
       " 0.02390933223068714,\n",
       " -0.016238756477832794,\n",
       " 0.0317004956305027,\n",
       " 0.02728424407541752,\n",
       " 0.052468810230493546,\n",
       " -0.04707096144556999,\n",
       " -0.058847490698099136,\n",
       " -0.0632081851363182,\n",
       " 0.04088858887553215,\n",
       " 0.04982799291610718,\n",
       " 0.10655169934034348,\n",
       " -0.07450234144926071,\n",
       " -0.012495442293584347,\n",
       " 0.01837068982422352,\n",
       " 0.03947409614920616,\n",
       " -0.024797888472676277,\n",
       " 0.014516280964016914,\n",
       " -0.03706915304064751,\n",
       " 0.020015738904476166,\n",
       " -4.8577076086075976e-05,\n",
       " 0.009866570122539997,\n",
       " 0.02483874000608921,\n",
       " -0.052458181977272034,\n",
       " 0.02931421808898449,\n",
       " -0.08719193190336227,\n",
       " -0.01449969969689846,\n",
       " 0.026019055396318436,\n",
       " -0.018746361136436462,\n",
       " -0.07620514184236526,\n",
       " 0.03504330292344093,\n",
       " 0.10363954305648804,\n",
       " -0.02805054374039173,\n",
       " 0.012718209996819496,\n",
       " -0.07632549852132797,\n",
       " -0.01865239255130291,\n",
       " 0.024976717308163643,\n",
       " 0.08144529908895493,\n",
       " 0.06875890493392944,\n",
       " -0.06405667215585709,\n",
       " -0.08389384299516678,\n",
       " 0.06136239320039749,\n",
       " -0.033545538783073425,\n",
       " -0.10615337640047073,\n",
       " -0.04008055850863457,\n",
       " 0.03253018110990524,\n",
       " 0.07662487775087357,\n",
       " -0.0730162188410759,\n",
       " 0.00033754485775716603,\n",
       " -0.040871694684028625,\n",
       " -0.07578853517770767,\n",
       " 0.027527635917067528,\n",
       " 0.0746254101395607,\n",
       " 0.01771724410355091,\n",
       " 0.09121844917535782,\n",
       " 0.11022017151117325,\n",
       " 0.0005697705782949924,\n",
       " 0.05146333947777748,\n",
       " -0.014551302418112755,\n",
       " 0.033232029527425766,\n",
       " 0.023792268708348274,\n",
       " -0.022889861837029457,\n",
       " 0.03893749788403511,\n",
       " 0.030206803232431412]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085cb9b7-d49a-4df9-9f49-8ba34d4c1f76",
   "metadata": {},
   "source": [
    "## Pinecone initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "903370e8-a179-495a-9e5d-7247fa217dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=\"a4dc4d0f-f863-48fe-bd65-a02a2118e0b8\")\n",
    "index_list = pc.list_indexes()\n",
    "index = pc.Index(\"herbalmedicinechatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38b0403b-6251-4aae-af24-7ac5250e04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given that our chunk size is 500, an ideal batch size for upserting to Pinecone would typically be smaller than your chunk size. The optimal batch size can vary depending on factors like your system's memory, network conditions, and Pinecone's current load. However, a good starting point would be a batch size between 100 to 200.\n",
    "batch_size = 100  # as advised\n",
    "for i in range(0, len(text_chunks), batch_size):\n",
    "    batch = text_chunks[i:i+batch_size]\n",
    "    ids = [f\"id_{j}\" for j in range(i, min(i+batch_size, len(text_chunks)))]\n",
    "    embeddings = embedding.embed_documents([chunk.page_content for chunk in batch])\n",
    "    metadata = [{\"text\": chunk.page_content} for chunk in batch]\n",
    "    \n",
    "    index.upsert(vectors=zip(ids, embeddings, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d9c205eb-b31b-4a47-ba72-4cf9acefa4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing our index\n",
    "query_vector = embedding.embed_query(\"what is Mexican bamboo\")\n",
    "results = index.query(vector=query_vector, top_k=3, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ed1fa580-f0b7-4def-bb41-8a5d4786960e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': 'id_7366',\n",
      "              'metadata': {'text': 'Mescal Bean (Texas Mountain Laurel); '\n",
      "                                   'Sophora \\n'\n",
      "                                   'secundiﬂora  (Ortega) Lag. ex DC. \\n'\n",
      "                                   '(Synonym: Broussonetia  secundiﬂora  \\n'\n",
      "                                   'Ortega), 499\\n'\n",
      "                                   'Mesquite (Ironwood); Prosopis juliﬂora  '\n",
      "                                   '(Sw.) DC., \\n'\n",
      "                                   '499\\n'\n",
      "                                   'Metel, Hindu Datura (Downy Thornapple, '\n",
      "                                   'Hindu \\n'\n",
      "                                   'Thornapple, Hoary Thornapple, Horn-of-\\n'\n",
      "                                   'Plenty, Purple Thornapple); Datura metel  '\n",
      "                                   'L \\n'\n",
      "                                   '(Synonyms: D. alba  Nees, D. chlorantha  \\n'\n",
      "                                   'Hook., D. fastuosa  L., D. metel  var. '\n",
      "                                   'fastuosa  \\n'\n",
      "                                   '(L.) Staff.), 500\\n'\n",
      "                                   'Mexican Calea, Dog’s Grass, Bitter Grass; '\n",
      "                                   'Calea'},\n",
      "              'score': 0.437223136,\n",
      "              'values': []},\n",
      "             {'id': 'id_7273',\n",
      "              'metadata': {'text': 'Chinese Cucumber (Chinese Snake Gourd); \\n'\n",
      "                                   'Trichosanthes kirilowii  Maxim., 186\\n'\n",
      "                                   'Chinese Motherwort; Leonurus japonicus  '\n",
      "                                   'Houtt. \\n'\n",
      "                                   '(Synonym: L. artemisia  (Lour.) S.Y . Hu, '\n",
      "                                   'L. \\n'\n",
      "                                   'heterophyllus  Sweet, L. sibiricus  auct. '\n",
      "                                   'pl., \\n'\n",
      "                                   'Stachys artemisia  Lour.), 187\\n'\n",
      "                                   'Chinese Olive (Java Almond, Kenari Nut '\n",
      "                                   'Tree); \\n'\n",
      "                                   'Canarium vulgare  Leenh. (Synonym: C. \\n'\n",
      "                                   'commune  auct.), 187\\n'\n",
      "                                   'Chinese Peony (Common Garden Peony, '\n",
      "                                   'White \\n'\n",
      "                                   'Peony); Paeonia lactiﬂora  Pall. '\n",
      "                                   '(Synonyms: \\n'\n",
      "                                   'P . edulis  Salisb., P . fragrans  '\n",
      "                                   'Redoute.), 188'},\n",
      "              'score': 0.396248221,\n",
      "              'values': []},\n",
      "             {'id': 'id_7289',\n",
      "              'metadata': {'text': 'Handbook of Medicinal Herbs 851\\n'\n",
      "                                   'Cypress Spurge; Euphorbia cyparissias  L., '\n",
      "                                   '238\\n'\n",
      "                                   'D\\n'\n",
      "                                   'Daffodil (Bunchﬂower Daffodil, Chinese '\n",
      "                                   'Sacred Lily, \\n'\n",
      "                                   'Polyanthus narcissus); Narcissus tazetta  '\n",
      "                                   'L. \\n'\n",
      "                                   '(Synonym: Narcissus canaliculatus  '\n",
      "                                   'Guss.), \\n'\n",
      "                                   '241\\n'\n",
      "                                   'Damiana (Mexican Holly); Turnera diffusa  '\n",
      "                                   'Willd. ex \\n'\n",
      "                                   'Schult. (Synonym: Turnera microphylla  \\n'\n",
      "                                   'Desv. ex Ham.), 242\\n'\n",
      "                                   'Dandelion (Lion’s Tooth); Taraxacum '\n",
      "                                   'ofﬁcinale  F. H. \\n'\n",
      "                                   'Wigg. group (Synonyms: Leontodon \\n'\n",
      "                                   'taraxacum  L., Taraxacum dens-leonis  '\n",
      "                                   'Desf., \\n'\n",
      "                                   'T. vulgare  Schrank), 243'},\n",
      "              'score': 0.375804424,\n",
      "              'values': []}],\n",
      " 'namespace': '',\n",
      " 'usage': {'read_units': 6}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e55cb7-c583-4303-9818-05fe73da6451",
   "metadata": {},
   "source": [
    "<b>Remarque:</b>\n",
    "this is not readable so we'll need to generate our correct answer using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b5afe-f936-4e4d-97c8-ea5e3176f230",
   "metadata": {},
   "source": [
    "## integrating LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c941340a-b312-46eb-8b94-73ffd1aa0a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "61541cc5-c8bc-42dc-a9c8-0d24b3142992",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT= PromptTemplate(template=prompt_template, input_variables=[\"context\",\"question\"])\n",
    "chain_type_kwargs={\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "343c437d-b70a-4932-8d63-211d3dd06060",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=CTransformers(model=\"./model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':522,\n",
    "                          'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8a739e51-5ad8-494b-bd79-b021f44585b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "import torch\n",
    "\n",
    "text_field = \"text\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "vectorstore = LangchainPinecone(index, model.encode, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b8155f69-69f0-4cae-b798-e064ebbf50fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9f11abec-822d-4e9d-92af-e394cd086eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={'k':3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "daacab6e-0735-40a8-a979-e5fb0c3d0514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mRetrievalQA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_chain_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'BaseLanguageModel'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchain_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'stuff'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchain_type_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[dict]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'BaseRetrievalQA'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m Load chain from chain type.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Herbal-Medicine-Chatbot-using-Llama2/HerbMedBot/lib/python3.8/site-packages/langchain/chains/retrieval_qa/base.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RetrievalQA.from_chain_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c8ca8755-7448-413b-847c-ad4b30c6d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"./model/llama-2-7b-chat.ggmlv3.q4_0.bin\"\n",
    "\n",
    "def format_results(results):\n",
    "    formatted_text = \"\"\n",
    "    for match in results['matches']:\n",
    "        text = match['metadata']['text']\n",
    "        formatted_text += f\"- {text}\\n\\n\"\n",
    "    return formatted_text\n",
    "\n",
    "\n",
    "def generate_answer(prompt, model):\n",
    "    if isinstance(prompt, str):\n",
    "        prompt = [prompt]\n",
    "    outputs = model.generate(prompt)\n",
    "    return outputs['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "055906ee-356b-4fdb-9ebd-482ea9307fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Prompt: what's a horsemint ?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     formatted_text \u001b[38;5;241m=\u001b[39m format_results(results)\n\u001b[1;32m     11\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBased on the following information, provide a comprehensive and readable explanation about:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mformatted_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#result=qa({\"question\":user_input})\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#print(\"Response:\", result[\"result\"])\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "Cell \u001b[0;32mIn[139], line 16\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     15\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m [prompt]\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Herbal-Medicine-Chatbot-using-Llama2/HerbMedBot/lib/python3.8/site-packages/langchain/llms/base.py:234\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    229\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    232\u001b[0m         dumpd(\u001b[38;5;28mself\u001b[39m), prompts, invocation_params\u001b[38;5;241m=\u001b[39mparams, options\u001b[38;5;241m=\u001b[39moptions\n\u001b[1;32m    233\u001b[0m     )\n\u001b[0;32m--> 234\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Herbal-Medicine-Chatbot-using-Llama2/HerbMedBot/lib/python3.8/site-packages/langchain/llms/base.py:178\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    177\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    179\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Herbal-Medicine-Chatbot-using-Llama2/HerbMedBot/lib/python3.8/site-packages/langchain/llms/base.py:165\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 165\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    173\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    174\u001b[0m         )\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Herbal-Medicine-Chatbot-using-Llama2/HerbMedBot/lib/python3.8/site-packages/langchain/llms/base.py:557\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m    556\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 557\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    560\u001b[0m     )\n\u001b[1;32m    561\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/Herbal-Medicine-Chatbot-using-Llama2/HerbMedBot/lib/python3.8/site-packages/langchain/llms/ctransformers.py:102\u001b[0m, in \u001b[0;36mCTransformers._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m text \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    101\u001b[0m _run_manager \u001b[38;5;241m=\u001b[39m run_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManagerForLLMRun\u001b[38;5;241m.\u001b[39mget_noop_manager()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt, stop\u001b[38;5;241m=\u001b[39mstop, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    103\u001b[0m     text\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[1;32m    104\u001b[0m     _run_manager\u001b[38;5;241m.\u001b[39mon_llm_new_token(chunk, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/Herbal-Medicine-Chatbot-using-Llama2/HerbMedBot/lib/python3.8/site-packages/ctransformers/llm.py:432\u001b[0m, in \u001b[0;36mLLM._stream\u001b[0;34m(self, prompt, max_new_tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, stop, reset)\u001b[0m\n\u001b[1;32m    430\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    431\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    433\u001b[0m     tokens,\n\u001b[1;32m    434\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    435\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m    436\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    437\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39mrepetition_penalty,\n\u001b[1;32m    438\u001b[0m     last_n_tokens\u001b[38;5;241m=\u001b[39mlast_n_tokens,\n\u001b[1;32m    439\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m    440\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    441\u001b[0m     threads\u001b[38;5;241m=\u001b[39mthreads,\n\u001b[1;32m    442\u001b[0m     reset\u001b[38;5;241m=\u001b[39mreset,\n\u001b[1;32m    443\u001b[0m ):\n\u001b[1;32m    444\u001b[0m     text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize([token])\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# https://github.com/abetlen/llama-cpp-python/blob/1a13d76c487df1c8560132d10bda62d6e2f4fa93/llama_cpp/llama.py#L686-L706\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;66;03m# Check if one of the stop sequences is part of the text.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Note that the stop sequence may not always be at the end of text.\u001b[39;00m\n",
      "File \u001b[0;32m~/Herbal-Medicine-Chatbot-using-Llama2/HerbMedBot/lib/python3.8/site-packages/ctransformers/llm.py:400\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, reset)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    393\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    394\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m    399\u001b[0m     )\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_eos_token(token):\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Herbal-Medicine-Chatbot-using-Llama2/HerbMedBot/lib/python3.8/site-packages/ctransformers/llm.py:307\u001b[0m, in \u001b[0;36mLLM.eval\u001b[0;34m(self, tokens, batch_size, threads)\u001b[0m\n\u001b[1;32m    305\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens)\n\u001b[1;32m    306\u001b[0m tokens \u001b[38;5;241m=\u001b[39m (c_int \u001b[38;5;241m*\u001b[39m n_tokens)(\u001b[38;5;241m*\u001b[39mtokens)\n\u001b[0;32m--> 307\u001b[0m status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctransformers_llm_batch_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m status:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to evaluate tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input=input(f\"Input Prompt:\")\n",
    "\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "        print(\"Exiting the chatbot. Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        query_vector = embedding.embed_query(user_input)\n",
    "        result = index.query(vector=query_vector, top_k=3, include_metadata=True)\n",
    "        formatted_text = format_results(results)\n",
    "        prompt = f\"Based on the following information, provide a comprehensive and readable explanation about:\\n\\n{formatted_text}\"\n",
    "        answer = generate_answer(prompt, llm)\n",
    "    #result=qa({\"question\":user_input})\n",
    "    #print(\"Response:\", result[\"result\"])\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9983a80d-16e7-44df-ade7-157151637118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "import pinecone\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the Pinecone vector store with correct embedding function\n",
    "vectorstore = LangchainPinecone(\n",
    "    index=index,\n",
    "    embedding_function=embedding.embed_query,\n",
    "    text_key=\"text\"\n",
    ")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "# Initialize the LLM (replace with the appropriate LLM initialization)\n",
    "llm = CTransformers(\n",
    "    model=\"./model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "    model_type=\"llama\",\n",
    "    config={'max_new_tokens': 522, 'temperature': 0.8}\n",
    ")\n",
    "\n",
    "# Initialize the RetrievalQA object\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "65f02bbb-a754-4aae-949f-408830407fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    try:\n",
    "        result = index.query(vector=query, top_k=8, include_metadata=True)\n",
    "        return result\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "37f53378-f6de-4db1-85d1-8681244ea4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Prompt:  what's a horsemint ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: Missing some input keys: {'query'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Prompt:  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the chatbot. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Main loop for the chatbot\n",
    "while True:\n",
    "    user_input = input(\"Input Prompt: \")\n",
    "\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "        print(\"Exiting the chatbot. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        query_vector = embedding.embed_query(user_input)\n",
    "        result = process_query(query_vector)\n",
    "        if result:\n",
    "            context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "            response = qa({\"context\": context, \"question\": user_input})\n",
    "            print(\"Response:\", response[\"result\"])\n",
    "        else:\n",
    "            print(\"No relevant documents found.\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ce833344-2d77-4c98-bdf8-7749b8208baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mRetrievalQA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_chain_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'BaseLanguageModel'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchain_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'stuff'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchain_type_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[dict]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'BaseRetrievalQA'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m Load chain from chain type.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Herbal-Medicine-Chatbot-using-Llama2/HerbMedBot/lib/python3.8/site-packages/langchain/chains/retrieval_qa/base.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RetrievalQA.from_chain_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a5483781-4a46-4d28-bff8-d91f7c3696f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = LangchainPinecone(index, embedding.embed_query, text_key=\"text\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "203c0a44-bc95-44bb-8461-826fb58762a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=retriever,\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\n",
    "        \"verbose\": True,\n",
    "        \"prompt\": PROMPT\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "54f510d1-3228-42d0-aa7e-09d3337e7fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['APIChain', 'AnalyzeDocumentChain', 'ChatVectorDBChain', 'ConstitutionalChain', 'ConversationChain', 'ConversationalRetrievalChain', 'FlareChain', 'GraphCypherQAChain', 'GraphQAChain', 'GraphSparqlQAChain', 'HugeGraphQAChain', 'HypotheticalDocumentEmbedder', 'KuzuQAChain', 'LLMBashChain', 'LLMChain', 'LLMCheckerChain', 'LLMMathChain', 'LLMRequestsChain', 'LLMRouterChain', 'LLMSummarizationCheckerChain', 'MapReduceChain', 'MapReduceDocumentsChain', 'MapRerankDocumentsChain', 'MultiPromptChain', 'MultiRetrievalQAChain', 'MultiRouteChain', 'NatBotChain', 'NebulaGraphQAChain', 'OpenAIModerationChain', 'OpenAPIEndpointChain', 'PALChain', 'QAGenerationChain', 'QAWithSourcesChain', 'ReduceDocumentsChain', 'RefineDocumentsChain', 'RetrievalQA', 'RetrievalQAWithSourcesChain', 'RouterChain', 'SQLDatabaseChain', 'SQLDatabaseSequentialChain', 'SequentialChain', 'SimpleSequentialChain', 'StuffDocumentsChain', 'TransformChain', 'VectorDBQA', 'VectorDBQAWithSourcesChain', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'api', 'base', 'combine_documents', 'constitutional_ai', 'conversation', 'conversational_retrieval', 'create_citation_fuzzy_match_chain', 'create_extraction_chain', 'create_extraction_chain_pydantic', 'create_qa_with_sources_chain', 'create_qa_with_structure_chain', 'create_tagging_chain', 'create_tagging_chain_pydantic', 'flare', 'graph_qa', 'hyde', 'llm', 'llm_bash', 'llm_checker', 'llm_math', 'llm_requests', 'llm_summarization_checker', 'load_chain', 'loading', 'mapreduce', 'moderation', 'natbot', 'openai_functions', 'pal', 'prompt_selector', 'qa_generation', 'qa_with_sources', 'question_answering', 'retrieval_qa', 'router', 'sequential', 'sql_database', 'transform']\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(dir(langchain.chains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dbf70976-6f95-4d27-a462-35268228667a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationalRetrievalChain\ncombine_docs_chain\n  Can't instantiate abstract class BaseCombineDocumentsChain with abstract methods acombine_docs, combine_docs (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m LangchainPinecone(index, model\u001b[38;5;241m.\u001b[39mencode, text_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Initialize the ConversationalRetrievalChain\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m conversational_qa \u001b[38;5;241m=\u001b[39m \u001b[43mConversationalRetrievalChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion_generator\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Herbal-Medicine-Chatbot-using-Llama2/HerbMedBot/lib/python3.8/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/Herbal-Medicine-Chatbot-using-Llama2/HerbMedBot/lib/python3.8/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ConversationalRetrievalChain\ncombine_docs_chain\n  Can't instantiate abstract class BaseCombineDocumentsChain with abstract methods acombine_docs, combine_docs (type=type_error)"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import LLMChain, AnalyzeDocumentChain, MapReduceChain\n",
    "\n",
    "# Define the LLMChain to combine documents\n",
    "combine_docs_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT\n",
    ")\n",
    "\n",
    "# Define the question generator (this can be a simple prompt or a model)\n",
    "question_generator = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=\"Generate a question from: {query}\", input_variables=[\"query\"])\n",
    ")\n",
    "\n",
    "# Initialize the Pinecone vectorstore\n",
    "vectorstore = LangchainPinecone(index, model.encode, text_key=\"text\")\n",
    "\n",
    "# Initialize the ConversationalRetrievalChain\n",
    "conversational_qa = ConversationalRetrievalChain(\n",
    "    combine_docs_chain=combine_docs_chain,\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 6}),\n",
    "    question_generator=question_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "17419bd9-600c-43be-aa48-361a13251329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "ValueError: The argument order for `query()` has changed; please use keyword arguments instead of positional arguments. Example: index.query(vector=[0.1, 0.2, 0.3], top_k=10, namespace='my_namespace')\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def answer_query(query):\n",
    "    try:\n",
    "        response = qa({\"query\": query})\n",
    "        return response[\"result\"]\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Test the chain with a query\n",
    "print(answer_query(\"What's a horsemint?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819efd33-3ec3-4670-9df7-d779d537ff3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
